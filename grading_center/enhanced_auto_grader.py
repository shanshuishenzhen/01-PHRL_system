#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„è‡ªåŠ¨é˜…å·ç³»ç»Ÿ

åŸºäºæœ€ä½³å®è·µçš„æ™ºèƒ½é˜…å·ç®—æ³•ï¼Œæ”¯æŒï¼š
- å¤šç§é¢˜å‹çš„æ™ºèƒ½è¯„åˆ†
- è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ
- å…³é”®è¯åŒ¹é…
- æ¨¡ç³ŠåŒ¹é…
- è¯„åˆ†è§„åˆ™é…ç½®
- è´¨é‡æ£€æŸ¥
"""

import os
import sys
import json
import re
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from difflib import SequenceMatcher
import jieba
import jieba.analyse


@dataclass
class GradingRule:
    """è¯„åˆ†è§„åˆ™é…ç½®"""
    question_type: str
    exact_match_score: float = 1.0
    partial_match_score: float = 0.5
    keyword_weight: float = 0.3
    similarity_threshold: float = 0.8
    case_sensitive: bool = False
    ignore_punctuation: bool = True
    keywords: List[str] = None
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = []


@dataclass
class GradingResult:
    """è¯„åˆ†ç»“æœ"""
    question_id: str
    question_type: str
    student_answer: str
    correct_answer: str
    max_score: float
    obtained_score: float
    score_ratio: float
    grading_method: str
    confidence: float
    feedback: str
    keywords_matched: List[str] = None
    similarity_score: float = 0.0
    
    def __post_init__(self):
        if self.keywords_matched is None:
            self.keywords_matched = []
        self.score_ratio = self.obtained_score / self.max_score if self.max_score > 0 else 0


class EnhancedAutoGrader:
    """å¢å¼ºçš„è‡ªåŠ¨é˜…å·å™¨"""
    
    def __init__(self, config_path: str = None):
        self.setup_logging()
        self.load_config(config_path)
        self.setup_jieba()
        
        # é»˜è®¤è¯„åˆ†è§„åˆ™
        self.default_rules = {
            "single_choice": GradingRule("single_choice", exact_match_score=1.0),
            "multiple_choice": GradingRule("multiple_choice", exact_match_score=1.0),
            "true_false": GradingRule("true_false", exact_match_score=1.0),
            "fill_blank": GradingRule("fill_blank", exact_match_score=1.0, 
                                    partial_match_score=0.7, similarity_threshold=0.9),
            "short_answer": GradingRule("short_answer", exact_match_score=1.0,
                                      partial_match_score=0.6, keyword_weight=0.4,
                                      similarity_threshold=0.7),
            "essay": GradingRule("essay", exact_match_score=1.0,
                               partial_match_score=0.5, keyword_weight=0.5,
                               similarity_threshold=0.6)
        }
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
            handlers=[
                logging.FileHandler(log_dir / "enhanced_grader.log", encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_config(self, config_path: str = None):
        """åŠ è½½é…ç½®"""
        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
            except Exception as e:
                self.logger.warning(f"åŠ è½½é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                self.config = {}
        else:
            self.config = {}
    
    def setup_jieba(self):
        """è®¾ç½®jiebaåˆ†è¯"""
        try:
            # æ·»åŠ è‡ªå®šä¹‰è¯å…¸
            custom_words = self.config.get("custom_words", [])
            for word in custom_words:
                jieba.add_word(word)
            
            # è®¾ç½®åœç”¨è¯
            self.stop_words = set(self.config.get("stop_words", [
                "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™"
            ]))
            
        except Exception as e:
            self.logger.warning(f"è®¾ç½®jiebaå¤±è´¥: {e}")
    
    def normalize_text(self, text: str, rule: GradingRule) -> str:
        """æ–‡æœ¬æ ‡å‡†åŒ–"""
        if not text:
            return ""
        
        text = str(text).strip()
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        if rule.ignore_punctuation:
            text = re.sub(r'[^\w\s]', '', text)
        
        # å¤§å°å†™å¤„ç†
        if not rule.case_sensitive:
            text = text.lower()
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        
        return text
    
    def extract_keywords(self, text: str, top_k: int = 10) -> List[str]:
        """æå–å…³é”®è¯"""
        try:
            # ä½¿ç”¨TF-IDFæå–å…³é”®è¯
            keywords = jieba.analyse.extract_tags(text, topK=top_k, withWeight=False)
            # è¿‡æ»¤åœç”¨è¯
            keywords = [kw for kw in keywords if kw not in self.stop_words]
            return keywords
        except Exception as e:
            self.logger.warning(f"æå–å…³é”®è¯å¤±è´¥: {e}")
            return []
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        try:
            # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
            similarity = SequenceMatcher(None, text1, text2).ratio()
            return similarity
        except Exception as e:
            self.logger.warning(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0
    
    def calculate_keyword_match_score(self, student_text: str, correct_text: str, 
                                    keywords: List[str] = None) -> Tuple[float, List[str]]:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        if not keywords:
            keywords = self.extract_keywords(correct_text)
        
        if not keywords:
            return 0.0, []
        
        student_keywords = self.extract_keywords(student_text)
        matched_keywords = []
        
        for keyword in keywords:
            if keyword in student_keywords or keyword in student_text:
                matched_keywords.append(keyword)
        
        match_ratio = len(matched_keywords) / len(keywords) if keywords else 0
        return match_ratio, matched_keywords
    
    def grade_single_choice(self, student_answer: str, correct_answer: str, 
                          max_score: float, rule: GradingRule) -> GradingResult:
        """è¯„åˆ†å•é€‰é¢˜"""
        student_norm = self.normalize_text(student_answer, rule)
        correct_norm = self.normalize_text(correct_answer, rule)
        
        if student_norm == correct_norm:
            score = max_score
            confidence = 1.0
            feedback = "ç­”æ¡ˆå®Œå…¨æ­£ç¡®"
        else:
            score = 0
            confidence = 1.0
            feedback = f"ç­”æ¡ˆé”™è¯¯ï¼Œæ­£ç¡®ç­”æ¡ˆæ˜¯: {correct_answer}"
        
        return GradingResult(
            question_id="", question_type="single_choice",
            student_answer=student_answer, correct_answer=correct_answer,
            max_score=max_score, obtained_score=score,
            grading_method="exact_match", confidence=confidence,
            feedback=feedback, score_ratio=score/max_score
        )
    
    def grade_multiple_choice(self, student_answer: Any, correct_answer: Any,
                            max_score: float, rule: GradingRule) -> GradingResult:
        """è¯„åˆ†å¤šé€‰é¢˜"""
        # å¤„ç†ä¸åŒæ ¼å¼çš„ç­”æ¡ˆ
        if isinstance(student_answer, str):
            student_set = set(student_answer.strip().upper())
        elif isinstance(student_answer, list):
            student_set = set(str(x).strip().upper() for x in student_answer)
        else:
            student_set = set()
        
        if isinstance(correct_answer, str):
            correct_set = set(correct_answer.strip().upper())
        elif isinstance(correct_answer, list):
            correct_set = set(str(x).strip().upper() for x in correct_answer)
        else:
            correct_set = set()
        
        if student_set == correct_set:
            score = max_score
            feedback = "ç­”æ¡ˆå®Œå…¨æ­£ç¡®"
            confidence = 1.0
        else:
            # éƒ¨åˆ†åˆ†æ•°è®¡ç®—
            intersection = student_set & correct_set
            union = student_set | correct_set
            
            if intersection:
                # æ ¹æ®äº¤é›†æ¯”ä¾‹ç»™åˆ†
                partial_ratio = len(intersection) / len(correct_set)
                score = max_score * partial_ratio * rule.partial_match_score
                feedback = f"éƒ¨åˆ†æ­£ç¡®ï¼Œå¾—åˆ° {len(intersection)}/{len(correct_set)} ä¸ªæ­£ç¡®é€‰é¡¹"
                confidence = 0.8
            else:
                score = 0
                feedback = f"ç­”æ¡ˆé”™è¯¯ï¼Œæ­£ç¡®ç­”æ¡ˆæ˜¯: {correct_answer}"
                confidence = 1.0
        
        return GradingResult(
            question_id="", question_type="multiple_choice",
            student_answer=str(student_answer), correct_answer=str(correct_answer),
            max_score=max_score, obtained_score=score,
            grading_method="set_comparison", confidence=confidence,
            feedback=feedback, score_ratio=score/max_score
        )
    
    def grade_fill_blank(self, student_answer: str, correct_answer: str,
                        max_score: float, rule: GradingRule) -> GradingResult:
        """è¯„åˆ†å¡«ç©ºé¢˜"""
        student_norm = self.normalize_text(student_answer, rule)
        correct_norm = self.normalize_text(correct_answer, rule)
        
        # ç²¾ç¡®åŒ¹é…
        if student_norm == correct_norm:
            score = max_score
            confidence = 1.0
            feedback = "ç­”æ¡ˆå®Œå…¨æ­£ç¡®"
            method = "exact_match"
        else:
            # ç›¸ä¼¼åº¦åŒ¹é…
            similarity = self.calculate_similarity(student_norm, correct_norm)
            
            if similarity >= rule.similarity_threshold:
                score = max_score * rule.partial_match_score
                confidence = similarity
                feedback = f"ç­”æ¡ˆåŸºæœ¬æ­£ç¡®ï¼ˆç›¸ä¼¼åº¦: {similarity:.2f}ï¼‰"
                method = "similarity_match"
            else:
                # å…³é”®è¯åŒ¹é…
                keyword_score, matched_keywords = self.calculate_keyword_match_score(
                    student_norm, correct_norm
                )
                
                if keyword_score > 0:
                    score = max_score * keyword_score * rule.keyword_weight
                    confidence = keyword_score
                    feedback = f"åŒ…å«éƒ¨åˆ†å…³é”®è¯: {', '.join(matched_keywords)}"
                    method = "keyword_match"
                else:
                    score = 0
                    confidence = 1.0
                    feedback = f"ç­”æ¡ˆé”™è¯¯ï¼Œæ­£ç¡®ç­”æ¡ˆæ˜¯: {correct_answer}"
                    method = "no_match"
        
        return GradingResult(
            question_id="", question_type="fill_blank",
            student_answer=student_answer, correct_answer=correct_answer,
            max_score=max_score, obtained_score=score,
            grading_method=method, confidence=confidence,
            feedback=feedback, similarity_score=similarity if 'similarity' in locals() else 0,
            score_ratio=score/max_score
        )
    
    def grade_question(self, question_id: str, student_answer: Any, correct_answer: Any,
                      question_type: str, max_score: float, 
                      custom_rule: GradingRule = None) -> GradingResult:
        """è¯„åˆ†å•ä¸ªé¢˜ç›®"""
        try:
            # è·å–è¯„åˆ†è§„åˆ™
            rule = custom_rule or self.default_rules.get(question_type, 
                                                       self.default_rules["essay"])
            
            # æ ¹æ®é¢˜å‹é€‰æ‹©è¯„åˆ†æ–¹æ³•
            if question_type == "single_choice":
                result = self.grade_single_choice(str(student_answer), str(correct_answer), 
                                                max_score, rule)
            elif question_type == "multiple_choice":
                result = self.grade_multiple_choice(student_answer, correct_answer, 
                                                  max_score, rule)
            elif question_type == "true_false":
                result = self.grade_single_choice(str(student_answer), str(correct_answer), 
                                                max_score, rule)
            elif question_type == "fill_blank":
                result = self.grade_fill_blank(str(student_answer), str(correct_answer), 
                                             max_score, rule)
            elif question_type in ["short_answer", "essay"]:
                result = self.grade_fill_blank(str(student_answer), str(correct_answer), 
                                             max_score, rule)
            else:
                # æœªçŸ¥é¢˜å‹ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†
                result = GradingResult(
                    question_id=question_id, question_type=question_type,
                    student_answer=str(student_answer), correct_answer=str(correct_answer),
                    max_score=max_score, obtained_score=max_score * 0.5,
                    grading_method="default", confidence=0.5,
                    feedback="æœªçŸ¥é¢˜å‹ï¼Œç»™äºˆé»˜è®¤åˆ†æ•°", score_ratio=0.5
                )
            
            # è®¾ç½®é¢˜ç›®ID
            result.question_id = question_id
            
            return result
            
        except Exception as e:
            self.logger.error(f"è¯„åˆ†é¢˜ç›® {question_id} å¤±è´¥: {e}")
            return GradingResult(
                question_id=question_id, question_type=question_type,
                student_answer=str(student_answer), correct_answer=str(correct_answer),
                max_score=max_score, obtained_score=0,
                grading_method="error", confidence=0,
                feedback=f"è¯„åˆ†å‡ºé”™: {str(e)}", score_ratio=0
            )
    
    def grade_exam(self, student_answers: Dict, correct_answers: Dict, 
                   custom_rules: Dict[str, GradingRule] = None) -> Dict:
        """è¯„åˆ†æ•´ä¸ªè€ƒè¯•"""
        try:
            results = []
            total_score = 0
            total_max_score = 0
            
            questions = correct_answers.get("questions", {})
            
            for question_id, correct_data in questions.items():
                student_answer = student_answers.get(question_id, "")
                correct_answer = correct_data.get("correct_answer", "")
                question_type = correct_data.get("question_type", "essay")
                max_score = float(correct_data.get("score", 10))
                
                # è·å–è‡ªå®šä¹‰è§„åˆ™
                custom_rule = None
                if custom_rules and question_id in custom_rules:
                    custom_rule = custom_rules[question_id]
                
                # è¯„åˆ†
                result = self.grade_question(
                    question_id, student_answer, correct_answer,
                    question_type, max_score, custom_rule
                )
                
                results.append(result.__dict__)
                total_score += result.obtained_score
                total_max_score += max_score
            
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            pass_score = total_max_score * 0.6  # 60%åŠæ ¼
            passed = total_score >= pass_score
            percentage = (total_score / total_max_score * 100) if total_max_score > 0 else 0
            
            return {
                "total_score": total_score,
                "total_max_score": total_max_score,
                "percentage": percentage,
                "passed": passed,
                "pass_score": pass_score,
                "question_results": results,
                "grading_summary": {
                    "total_questions": len(questions),
                    "graded_questions": len(results),
                    "average_confidence": sum(r["confidence"] for r in results) / len(results) if results else 0,
                    "grading_time": time.time()
                }
            }
            
        except Exception as e:
            self.logger.error(f"è¯„åˆ†è€ƒè¯•å¤±è´¥: {e}")
            return {
                "error": str(e),
                "total_score": 0,
                "total_max_score": 0,
                "percentage": 0,
                "passed": False
            }


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•å¢å¼ºé˜…å·ç³»ç»Ÿ"""
    grader = EnhancedAutoGrader()
    
    # æµ‹è¯•æ•°æ®
    student_answers = {
        "q1": "B",
        "q2": ["A", "C"],
        "q3": "Pythonæ˜¯ä¸€ç§è§£é‡Šå‹ç¼–ç¨‹è¯­è¨€",
        "q4": "é¢å‘å¯¹è±¡ç¼–ç¨‹çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬å°è£…ã€ç»§æ‰¿å’Œå¤šæ€"
    }
    
    correct_answers = {
        "questions": {
            "q1": {
                "correct_answer": "B",
                "question_type": "single_choice",
                "score": 10
            },
            "q2": {
                "correct_answer": ["A", "C"],
                "question_type": "multiple_choice", 
                "score": 15
            },
            "q3": {
                "correct_answer": "Pythonæ˜¯è§£é‡Šå‹è¯­è¨€",
                "question_type": "fill_blank",
                "score": 10
            },
            "q4": {
                "correct_answer": "é¢å‘å¯¹è±¡ç¼–ç¨‹çš„ç‰¹ç‚¹æ˜¯å°è£…ã€ç»§æ‰¿ã€å¤šæ€",
                "question_type": "short_answer",
                "score": 20
            }
        }
    }
    
    # æ‰§è¡Œè¯„åˆ†
    result = grader.grade_exam(student_answers, correct_answers)
    
    print("ğŸ¯ å¢å¼ºè‡ªåŠ¨é˜…å·ç»“æœ:")
    print(f"æ€»åˆ†: {result['total_score']:.1f}/{result['total_max_score']}")
    print(f"ç™¾åˆ†æ¯”: {result['percentage']:.1f}%")
    print(f"æ˜¯å¦åŠæ ¼: {'æ˜¯' if result['passed'] else 'å¦'}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {result['grading_summary']['average_confidence']:.2f}")
    
    print("\nè¯¦ç»†è¯„åˆ†:")
    for q_result in result['question_results']:
        print(f"é¢˜ç›® {q_result['question_id']}: {q_result['obtained_score']:.1f}/{q_result['max_score']} "
              f"({q_result['grading_method']}, ç½®ä¿¡åº¦: {q_result['confidence']:.2f})")
        print(f"  åé¦ˆ: {q_result['feedback']}")


if __name__ == "__main__":
    main()
