#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜…å·è´¨é‡ç›‘æ§ç³»ç»Ÿ

ç›‘æ§å’Œåˆ†æé˜…å·è´¨é‡ï¼ŒåŒ…æ‹¬ï¼š
- è¯„åˆ†ä¸€è‡´æ€§åˆ†æ
- å¼‚å¸¸è¯„åˆ†æ£€æµ‹
- è¯„åˆ†å‘˜è¡¨ç°åˆ†æ
- è´¨é‡æŠ¥å‘Šç”Ÿæˆ
- å®æ—¶ç›‘æ§é¢„è­¦
"""

import json
import logging
import statistics
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import numpy as np


@dataclass
class GradingRecord:
    """è¯„åˆ†è®°å½•"""
    record_id: str
    exam_id: str
    question_id: str
    student_id: str
    grader_id: str
    score: float
    max_score: float
    grading_time: str
    confidence: float = 1.0
    comments: str = ""
    grading_method: str = "manual"
    
    @property
    def score_ratio(self) -> float:
        return self.score / self.max_score if self.max_score > 0 else 0


@dataclass
class QualityMetrics:
    """è´¨é‡æŒ‡æ ‡"""
    consistency_score: float  # ä¸€è‡´æ€§åˆ†æ•° (0-1)
    reliability_score: float  # å¯é æ€§åˆ†æ•° (0-1)
    efficiency_score: float   # æ•ˆç‡åˆ†æ•° (0-1)
    accuracy_score: float     # å‡†ç¡®æ€§åˆ†æ•° (0-1)
    overall_score: float      # æ€»ä½“è´¨é‡åˆ†æ•° (0-1)
    
    anomaly_count: int = 0
    total_records: int = 0
    average_grading_time: float = 0.0
    score_variance: float = 0.0


@dataclass
class GraderPerformance:
    """è¯„åˆ†å‘˜è¡¨ç°"""
    grader_id: str
    grader_name: str
    total_graded: int
    average_score: float
    score_variance: float
    average_time: float
    consistency_with_others: float
    accuracy_rate: float
    quality_metrics: QualityMetrics
    last_active: str


class QualityMonitor:
    """é˜…å·è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self, data_dir: str = "quality_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        self.setup_logging()
        self.grading_records: List[GradingRecord] = []
        self.quality_thresholds = self.load_quality_thresholds()
        
        # åŠ è½½å†å²æ•°æ®
        self.load_grading_records()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
            handlers=[
                logging.FileHandler(log_dir / "quality_monitor.log", encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_quality_thresholds(self) -> Dict:
        """åŠ è½½è´¨é‡é˜ˆå€¼é…ç½®"""
        default_thresholds = {
            "consistency_threshold": 0.8,      # ä¸€è‡´æ€§é˜ˆå€¼
            "reliability_threshold": 0.85,     # å¯é æ€§é˜ˆå€¼
            "efficiency_threshold": 0.7,       # æ•ˆç‡é˜ˆå€¼
            "accuracy_threshold": 0.9,         # å‡†ç¡®æ€§é˜ˆå€¼
            "score_variance_threshold": 0.15,  # åˆ†æ•°æ–¹å·®é˜ˆå€¼
            "time_variance_threshold": 0.3,    # æ—¶é—´æ–¹å·®é˜ˆå€¼
            "anomaly_threshold": 0.05,         # å¼‚å¸¸æ¯”ä¾‹é˜ˆå€¼
            "min_grading_time": 30,            # æœ€å°è¯„åˆ†æ—¶é—´(ç§’)
            "max_grading_time": 1800,          # æœ€å¤§è¯„åˆ†æ—¶é—´(ç§’)
        }
        
        threshold_file = self.data_dir / "quality_thresholds.json"
        if threshold_file.exists():
            try:
                with open(threshold_file, 'r', encoding='utf-8') as f:
                    custom_thresholds = json.load(f)
                default_thresholds.update(custom_thresholds)
            except Exception as e:
                self.logger.warning(f"åŠ è½½è´¨é‡é˜ˆå€¼å¤±è´¥: {e}")
        
        return default_thresholds
    
    def load_grading_records(self):
        """åŠ è½½è¯„åˆ†è®°å½•"""
        try:
            records_file = self.data_dir / "grading_records.json"
            if records_file.exists():
                with open(records_file, 'r', encoding='utf-8') as f:
                    records_data = json.load(f)
                
                self.grading_records = [
                    GradingRecord(**record) for record in records_data
                ]
                
                self.logger.info(f"åŠ è½½è¯„åˆ†è®°å½•: {len(self.grading_records)} æ¡")
        except Exception as e:
            self.logger.error(f"åŠ è½½è¯„åˆ†è®°å½•å¤±è´¥: {e}")
    
    def save_grading_records(self):
        """ä¿å­˜è¯„åˆ†è®°å½•"""
        try:
            records_file = self.data_dir / "grading_records.json"
            records_data = [asdict(record) for record in self.grading_records]
            
            with open(records_file, 'w', encoding='utf-8') as f:
                json.dump(records_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¯„åˆ†è®°å½•å¤±è´¥: {e}")
    
    def add_grading_record(self, record: GradingRecord):
        """æ·»åŠ è¯„åˆ†è®°å½•"""
        self.grading_records.append(record)
        
        # å®æ—¶è´¨é‡æ£€æŸ¥
        self.check_real_time_quality(record)
        
        # å®šæœŸä¿å­˜
        if len(self.grading_records) % 100 == 0:
            self.save_grading_records()
    
    def check_real_time_quality(self, record: GradingRecord):
        """å®æ—¶è´¨é‡æ£€æŸ¥"""
        warnings = []
        
        # æ£€æŸ¥è¯„åˆ†æ—¶é—´å¼‚å¸¸
        try:
            grading_time = self.parse_grading_time(record.grading_time)
            if grading_time < self.quality_thresholds["min_grading_time"]:
                warnings.append(f"è¯„åˆ†æ—¶é—´è¿‡çŸ­: {grading_time}ç§’")
            elif grading_time > self.quality_thresholds["max_grading_time"]:
                warnings.append(f"è¯„åˆ†æ—¶é—´è¿‡é•¿: {grading_time}ç§’")
        except:
            pass
        
        # æ£€æŸ¥åˆ†æ•°å¼‚å¸¸
        if record.score > record.max_score:
            warnings.append(f"åˆ†æ•°è¶…å‡ºæœ€å¤§å€¼: {record.score}/{record.max_score}")
        
        if record.score < 0:
            warnings.append(f"åˆ†æ•°ä¸ºè´Ÿå€¼: {record.score}")
        
        # æ£€æŸ¥ç½®ä¿¡åº¦å¼‚å¸¸
        if record.confidence < 0.5:
            warnings.append(f"ç½®ä¿¡åº¦è¿‡ä½: {record.confidence}")
        
        # å‘å‡ºè­¦å‘Š
        if warnings:
            self.logger.warning(f"è¯„åˆ†è´¨é‡è­¦å‘Š - è®°å½•ID: {record.record_id}, "
                              f"è¯„åˆ†å‘˜: {record.grader_id}, è­¦å‘Š: {'; '.join(warnings)}")
    
    def parse_grading_time(self, grading_time_str: str) -> float:
        """è§£æè¯„åˆ†æ—¶é—´"""
        # ç®€åŒ–å®ç°ï¼Œå‡è®¾grading_timeæ˜¯ISOæ ¼å¼çš„æ—¶é—´æˆ³
        # å®é™…åº”è¯¥æ ¹æ®å…·ä½“æ ¼å¼è§£æ
        return 60.0  # é»˜è®¤60ç§’
    
    def calculate_consistency_score(self, question_records: List[GradingRecord]) -> float:
        """è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°"""
        if len(question_records) < 2:
            return 1.0
        
        # è®¡ç®—åˆ†æ•°æ¯”ä¾‹çš„æ ‡å‡†å·®
        score_ratios = [record.score_ratio for record in question_records]
        variance = statistics.variance(score_ratios) if len(score_ratios) > 1 else 0
        
        # è½¬æ¢ä¸ºä¸€è‡´æ€§åˆ†æ•° (æ–¹å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜)
        consistency = max(0, 1 - variance / self.quality_thresholds["score_variance_threshold"])
        return min(1.0, consistency)
    
    def detect_anomalies(self, records: List[GradingRecord]) -> List[GradingRecord]:
        """æ£€æµ‹å¼‚å¸¸è¯„åˆ†"""
        if len(records) < 3:
            return []
        
        anomalies = []
        score_ratios = [record.score_ratio for record in records]
        
        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        q1 = np.percentile(score_ratios, 25)
        q3 = np.percentile(score_ratios, 75)
        iqr = q3 - q1
        
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        for record in records:
            if record.score_ratio < lower_bound or record.score_ratio > upper_bound:
                anomalies.append(record)
        
        return anomalies
    
    def analyze_grader_performance(self, grader_id: str, 
                                 time_period: int = 30) -> GraderPerformance:
        """åˆ†æè¯„åˆ†å‘˜è¡¨ç°"""
        # è·å–æŒ‡å®šæ—¶é—´æ®µå†…çš„è¯„åˆ†è®°å½•
        cutoff_date = datetime.now() - timedelta(days=time_period)
        
        grader_records = [
            record for record in self.grading_records
            if record.grader_id == grader_id
        ]
        
        if not grader_records:
            return GraderPerformance(
                grader_id=grader_id,
                grader_name=f"è¯„åˆ†å‘˜_{grader_id}",
                total_graded=0,
                average_score=0,
                score_variance=0,
                average_time=0,
                consistency_with_others=0,
                accuracy_rate=0,
                quality_metrics=QualityMetrics(0, 0, 0, 0, 0),
                last_active=""
            )
        
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
        scores = [record.score_ratio for record in grader_records]
        average_score = statistics.mean(scores)
        score_variance = statistics.variance(scores) if len(scores) > 1 else 0
        
        # è®¡ç®—ä¸€è‡´æ€§
        consistency_scores = []
        for record in grader_records:
            same_question_records = [
                r for r in self.grading_records
                if r.question_id == record.question_id and r.student_id == record.student_id
            ]
            if len(same_question_records) > 1:
                consistency = self.calculate_consistency_score(same_question_records)
                consistency_scores.append(consistency)
        
        consistency_with_others = statistics.mean(consistency_scores) if consistency_scores else 1.0
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        quality_metrics = QualityMetrics(
            consistency_score=consistency_with_others,
            reliability_score=min(1.0, 1 - score_variance),
            efficiency_score=0.8,  # ç®€åŒ–è®¡ç®—
            accuracy_score=0.9,    # éœ€è¦ä¸æ ‡å‡†ç­”æ¡ˆæ¯”è¾ƒ
            overall_score=0.0,
            total_records=len(grader_records),
            score_variance=score_variance
        )
        
        quality_metrics.overall_score = (
            quality_metrics.consistency_score * 0.3 +
            quality_metrics.reliability_score * 0.3 +
            quality_metrics.efficiency_score * 0.2 +
            quality_metrics.accuracy_score * 0.2
        )
        
        return GraderPerformance(
            grader_id=grader_id,
            grader_name=f"è¯„åˆ†å‘˜_{grader_id}",
            total_graded=len(grader_records),
            average_score=average_score,
            score_variance=score_variance,
            average_time=0,  # éœ€è¦è®¡ç®—å®é™…æ—¶é—´
            consistency_with_others=consistency_with_others,
            accuracy_rate=0.9,  # ç®€åŒ–
            quality_metrics=quality_metrics,
            last_active=grader_records[-1].grading_time if grader_records else ""
        )
    
    def generate_quality_report(self, exam_id: str = None) -> Dict:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        # ç­›é€‰è®°å½•
        if exam_id:
            records = [r for r in self.grading_records if r.exam_id == exam_id]
        else:
            records = self.grading_records
        
        if not records:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°è¯„åˆ†è®°å½•"}
        
        # æ€»ä½“ç»Ÿè®¡
        total_records = len(records)
        unique_graders = len(set(record.grader_id for record in records))
        unique_questions = len(set(record.question_id for record in records))
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies = self.detect_anomalies(records)
        anomaly_rate = len(anomalies) / total_records if total_records > 0 else 0
        
        # è®¡ç®—æ€»ä½“è´¨é‡æŒ‡æ ‡
        all_scores = [record.score_ratio for record in records]
        overall_variance = statistics.variance(all_scores) if len(all_scores) > 1 else 0
        
        # æŒ‰é¢˜ç›®åˆ†æä¸€è‡´æ€§
        question_consistency = {}
        for question_id in set(record.question_id for record in records):
            question_records = [r for r in records if r.question_id == question_id]
            if len(question_records) > 1:
                consistency = self.calculate_consistency_score(question_records)
                question_consistency[question_id] = consistency
        
        average_consistency = statistics.mean(question_consistency.values()) if question_consistency else 1.0
        
        # è¯„åˆ†å‘˜è¡¨ç°åˆ†æ
        grader_performances = {}
        for grader_id in set(record.grader_id for record in records):
            performance = self.analyze_grader_performance(grader_id)
            grader_performances[grader_id] = asdict(performance)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "report_id": f"quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "generated_at": datetime.now().isoformat(),
            "exam_id": exam_id,
            "summary": {
                "total_records": total_records,
                "unique_graders": unique_graders,
                "unique_questions": unique_questions,
                "anomaly_count": len(anomalies),
                "anomaly_rate": anomaly_rate,
                "overall_variance": overall_variance,
                "average_consistency": average_consistency
            },
            "quality_metrics": {
                "consistency_score": average_consistency,
                "reliability_score": max(0, 1 - overall_variance),
                "anomaly_score": max(0, 1 - anomaly_rate),
                "overall_quality": (average_consistency + max(0, 1 - overall_variance) + max(0, 1 - anomaly_rate)) / 3
            },
            "question_analysis": question_consistency,
            "grader_performances": grader_performances,
            "anomalies": [asdict(anomaly) for anomaly in anomalies[:10]],  # åªæ˜¾ç¤ºå‰10ä¸ªå¼‚å¸¸
            "recommendations": self.generate_recommendations(
                average_consistency, overall_variance, anomaly_rate
            )
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.data_dir / f"quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report
    
    def generate_recommendations(self, consistency: float, variance: float, 
                               anomaly_rate: float) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if consistency < self.quality_thresholds["consistency_threshold"]:
            recommendations.append("è¯„åˆ†ä¸€è‡´æ€§è¾ƒä½ï¼Œå»ºè®®åŠ å¼ºè¯„åˆ†å‘˜åŸ¹è®­å’Œæ ‡å‡†åŒ–")
        
        if variance > self.quality_thresholds["score_variance_threshold"]:
            recommendations.append("è¯„åˆ†æ–¹å·®è¾ƒå¤§ï¼Œå»ºè®®åˆ¶å®šæ›´è¯¦ç»†çš„è¯„åˆ†æ ‡å‡†")
        
        if anomaly_rate > self.quality_thresholds["anomaly_threshold"]:
            recommendations.append("å¼‚å¸¸è¯„åˆ†è¾ƒå¤šï¼Œå»ºè®®å¢åŠ è´¨é‡æ£€æŸ¥å’Œå¤æ ¸æœºåˆ¶")
        
        if not recommendations:
            recommendations.append("è¯„åˆ†è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰æ ‡å‡†")
        
        return recommendations


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•è´¨é‡ç›‘æ§ç³»ç»Ÿ"""
    monitor = QualityMonitor()
    
    # æ·»åŠ æµ‹è¯•æ•°æ®
    test_records = [
        GradingRecord("r1", "exam1", "q1", "s1", "grader1", 8.5, 10, "2024-01-01T10:00:00Z", 0.9),
        GradingRecord("r2", "exam1", "q1", "s1", "grader2", 8.0, 10, "2024-01-01T10:05:00Z", 0.8),
        GradingRecord("r3", "exam1", "q1", "s1", "grader3", 9.0, 10, "2024-01-01T10:10:00Z", 0.95),
        GradingRecord("r4", "exam1", "q2", "s1", "grader1", 7.0, 10, "2024-01-01T10:15:00Z", 0.7),
        GradingRecord("r5", "exam1", "q2", "s1", "grader2", 6.5, 10, "2024-01-01T10:20:00Z", 0.6),
    ]
    
    for record in test_records:
        monitor.add_grading_record(record)
    
    print("ğŸ“Š é˜…å·è´¨é‡ç›‘æ§ç³»ç»Ÿæµ‹è¯•")
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    report = monitor.generate_quality_report("exam1")
    
    print(f"\nè´¨é‡æŠ¥å‘Šæ‘˜è¦:")
    print(f"æ€»è®°å½•æ•°: {report['summary']['total_records']}")
    print(f"è¯„åˆ†å‘˜æ•°: {report['summary']['unique_graders']}")
    print(f"é¢˜ç›®æ•°: {report['summary']['unique_questions']}")
    print(f"å¼‚å¸¸ç‡: {report['summary']['anomaly_rate']:.2%}")
    print(f"å¹³å‡ä¸€è‡´æ€§: {report['summary']['average_consistency']:.3f}")
    print(f"æ€»ä½“è´¨é‡: {report['quality_metrics']['overall_quality']:.3f}")
    
    print(f"\næ”¹è¿›å»ºè®®:")
    for rec in report['recommendations']:
        print(f"- {rec}")


if __name__ == "__main__":
    main()
