#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„æ—¥å¿—ç®¡ç†ç³»ç»Ÿ

æä¾›ç»Ÿä¸€çš„æ—¥å¿—ç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- å¤šçº§åˆ«æ—¥å¿—è®°å½•
- æ–‡ä»¶è½®è½¬å’Œå‹ç¼©
- ç»“æ„åŒ–æ—¥å¿—
- æ—¥å¿—èšåˆå’Œåˆ†æ
- è¿œç¨‹æ—¥å¿—å‘é€
- æ€§èƒ½ç›‘æ§
- æ—¥å¿—æœç´¢å’Œè¿‡æ»¤
"""

import os
import sys
import json
import time
import logging
import logging.handlers
import threading
import sqlite3
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, asdict
from queue import Queue, Empty
import traceback


@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®"""
    timestamp: str
    level: str
    logger_name: str
    module: str
    function: str
    line_number: int
    message: str
    extra_data: Dict[str, Any] = None
    trace_id: str = None
    user_id: str = None
    
    def __post_init__(self):
        if self.extra_data is None:
            self.extra_data = {}


class StructuredFormatter(logging.Formatter):
    """ç»“æ„åŒ–æ—¥å¿—æ ¼å¼åŒ–å™¨"""
    
    def format(self, record):
        """æ ¼å¼åŒ–æ—¥å¿—è®°å½•"""
        log_entry = {
            "timestamp": datetime.fromtimestamp(record.created).isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
            "message": record.getMessage(),
            "process_id": record.process,
            "thread_id": record.thread
        }
        
        # æ·»åŠ é¢å¤–æ•°æ®
        if hasattr(record, 'extra_data'):
            log_entry["extra"] = record.extra_data
        
        if hasattr(record, 'trace_id'):
            log_entry["trace_id"] = record.trace_id
        
        if hasattr(record, 'user_id'):
            log_entry["user_id"] = record.user_id
        
        # æ·»åŠ å¼‚å¸¸ä¿¡æ¯
        if record.exc_info:
            log_entry["exception"] = self.formatException(record.exc_info)
        
        return json.dumps(log_entry, ensure_ascii=False)


class DatabaseLogHandler(logging.Handler):
    """æ•°æ®åº“æ—¥å¿—å¤„ç†å™¨"""
    
    def __init__(self, db_path: str):
        super().__init__()
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(exist_ok=True)
        self.init_database()
        
        # æ‰¹é‡å†™å…¥é˜Ÿåˆ—
        self.log_queue = Queue()
        self.batch_size = 100
        self.flush_interval = 5  # ç§’
        
        # å¯åŠ¨æ‰¹é‡å†™å…¥çº¿ç¨‹
        self.running = True
        self.writer_thread = threading.Thread(target=self._batch_writer, daemon=True)
        self.writer_thread.start()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT,
                    level TEXT,
                    logger_name TEXT,
                    module TEXT,
                    function TEXT,
                    line_number INTEGER,
                    message TEXT,
                    extra_data TEXT,
                    trace_id TEXT,
                    user_id TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON logs(timestamp)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_level ON logs(level)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_logger ON logs(logger_name)")
    
    def emit(self, record):
        """å‘é€æ—¥å¿—è®°å½•"""
        try:
            log_entry = LogEntry(
                timestamp=datetime.fromtimestamp(record.created).isoformat(),
                level=record.levelname,
                logger_name=record.name,
                module=record.module,
                function=record.funcName,
                line_number=record.lineno,
                message=record.getMessage(),
                extra_data=getattr(record, 'extra_data', {}),
                trace_id=getattr(record, 'trace_id', None),
                user_id=getattr(record, 'user_id', None)
            )
            
            self.log_queue.put(log_entry)
            
        except Exception:
            self.handleError(record)
    
    def _batch_writer(self):
        """æ‰¹é‡å†™å…¥çº¿ç¨‹"""
        batch = []
        last_flush = time.time()
        
        while self.running:
            try:
                # è·å–æ—¥å¿—æ¡ç›®
                try:
                    entry = self.log_queue.get(timeout=1)
                    batch.append(entry)
                except Empty:
                    pass
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
                now = time.time()
                should_flush = (
                    len(batch) >= self.batch_size or
                    (batch and now - last_flush >= self.flush_interval)
                )
                
                if should_flush:
                    self._flush_batch(batch)
                    batch.clear()
                    last_flush = now
                    
            except Exception as e:
                print(f"æ‰¹é‡å†™å…¥æ—¥å¿—å¤±è´¥: {e}")
    
    def _flush_batch(self, batch: List[LogEntry]):
        """åˆ·æ–°æ‰¹æ¬¡åˆ°æ•°æ®åº“"""
        if not batch:
            return
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.executemany("""
                    INSERT INTO logs 
                    (timestamp, level, logger_name, module, function, line_number, 
                     message, extra_data, trace_id, user_id)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, [
                    (
                        entry.timestamp, entry.level, entry.logger_name,
                        entry.module, entry.function, entry.line_number,
                        entry.message, json.dumps(entry.extra_data),
                        entry.trace_id, entry.user_id
                    )
                    for entry in batch
                ])
        except Exception as e:
            print(f"å†™å…¥æ—¥å¿—åˆ°æ•°æ®åº“å¤±è´¥: {e}")


class EnhancedLogManager:
    """å¢å¼ºçš„æ—¥å¿—ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = "config/logging.json"):
        self.config_path = Path(config_path)
        self.load_config()
        
        # æ—¥å¿—å™¨æ³¨å†Œè¡¨
        self.loggers: Dict[str, logging.Logger] = {}
        
        # æ—¥å¿—å¤„ç†å™¨
        self.handlers: List[logging.Handler] = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_logs": 0,
            "logs_by_level": {"DEBUG": 0, "INFO": 0, "WARNING": 0, "ERROR": 0, "CRITICAL": 0},
            "logs_by_logger": {},
            "start_time": time.time()
        }
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.db_path = Path("data/logs.db")
        self.init_log_database()
    
    def load_config(self):
        """åŠ è½½é…ç½®"""
        if self.config_path.exists():
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
            except Exception as e:
                print(f"åŠ è½½æ—¥å¿—é…ç½®å¤±è´¥: {e}")
                self.config = self.get_default_config()
        else:
            self.config = self.get_default_config()
            self.save_config()
    
    def get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "standard": {
                    "format": "%(asctime)s [%(levelname)8s] %(name)s: %(message)s",
                    "datefmt": "%Y-%m-%d %H:%M:%S"
                },
                "detailed": {
                    "format": "%(asctime)s [%(levelname)8s] %(name)s:%(funcName)s:%(lineno)d: %(message)s",
                    "datefmt": "%Y-%m-%d %H:%M:%S"
                },
                "json": {
                    "class": "common.enhanced_logger.StructuredFormatter"
                }
            },
            "handlers": {
                "console": {
                    "class": "logging.StreamHandler",
                    "level": "INFO",
                    "formatter": "standard",
                    "stream": "ext://sys.stdout"
                },
                "file": {
                    "class": "logging.handlers.RotatingFileHandler",
                    "level": "DEBUG",
                    "formatter": "detailed",
                    "filename": "logs/app.log",
                    "maxBytes": 10485760,
                    "backupCount": 5,
                    "encoding": "utf-8"
                },
                "database": {
                    "class": "common.enhanced_logger.DatabaseLogHandler",
                    "level": "INFO",
                    "db_path": "data/logs.db"
                }
            },
            "loggers": {
                "": {
                    "level": "DEBUG",
                    "handlers": ["console", "file", "database"],
                    "propagate": False
                }
            },
            "settings": {
                "log_retention_days": 30,
                "max_log_file_size": "10MB",
                "compression_enabled": True,
                "remote_logging_enabled": False,
                "performance_logging": True
            }
        }
    
    def save_config(self):
        """ä¿å­˜é…ç½®"""
        self.config_path.parent.mkdir(exist_ok=True)
        with open(self.config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
    
    def init_log_database(self):
        """åˆå§‹åŒ–æ—¥å¿—æ•°æ®åº“"""
        self.db_path.parent.mkdir(exist_ok=True)
        
        with sqlite3.connect(self.db_path) as conn:
            # æ—¥å¿—ç»Ÿè®¡è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS log_stats (
                    date TEXT PRIMARY KEY,
                    total_logs INTEGER,
                    debug_logs INTEGER,
                    info_logs INTEGER,
                    warning_logs INTEGER,
                    error_logs INTEGER,
                    critical_logs INTEGER
                )
            """)
    
    def setup_logger(self, name: str, level: str = None, 
                    extra_handlers: List[logging.Handler] = None) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—å™¨"""
        if name in self.loggers:
            return self.loggers[name]
        
        logger = logging.getLogger(name)
        
        # è®¾ç½®çº§åˆ«
        log_level = level or self.config["loggers"][""]["level"]
        logger.setLevel(getattr(logging, log_level.upper(), logging.INFO))
        
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        logger.handlers.clear()
        
        # æ·»åŠ é…ç½®çš„å¤„ç†å™¨
        for handler_name in self.config["loggers"][""]["handlers"]:
            handler = self._create_handler(handler_name)
            if handler:
                logger.addHandler(handler)
        
        # æ·»åŠ é¢å¤–å¤„ç†å™¨
        if extra_handlers:
            for handler in extra_handlers:
                logger.addHandler(handler)
        
        # æ·»åŠ ç»Ÿè®¡è¿‡æ»¤å™¨
        logger.addFilter(self._create_stats_filter())
        
        self.loggers[name] = logger
        return logger
    
    def _create_handler(self, handler_name: str) -> Optional[logging.Handler]:
        """åˆ›å»ºå¤„ç†å™¨"""
        try:
            handler_config = self.config["handlers"][handler_name]
            handler_class = handler_config["class"]
            
            if handler_class == "logging.StreamHandler":
                handler = logging.StreamHandler()
            elif handler_class == "logging.handlers.RotatingFileHandler":
                log_file = Path(handler_config["filename"])
                log_file.parent.mkdir(exist_ok=True)
                handler = logging.handlers.RotatingFileHandler(
                    filename=log_file,
                    maxBytes=handler_config["maxBytes"],
                    backupCount=handler_config["backupCount"],
                    encoding=handler_config.get("encoding", "utf-8")
                )
            elif handler_class == "common.enhanced_logger.DatabaseLogHandler":
                handler = DatabaseLogHandler(handler_config["db_path"])
            else:
                return None
            
            # è®¾ç½®çº§åˆ«
            handler.setLevel(getattr(logging, handler_config["level"].upper()))
            
            # è®¾ç½®æ ¼å¼åŒ–å™¨
            formatter_name = handler_config.get("formatter", "standard")
            formatter = self._create_formatter(formatter_name)
            if formatter:
                handler.setFormatter(formatter)
            
            return handler
            
        except Exception as e:
            print(f"åˆ›å»ºå¤„ç†å™¨å¤±è´¥ {handler_name}: {e}")
            return None
    
    def _create_formatter(self, formatter_name: str) -> Optional[logging.Formatter]:
        """åˆ›å»ºæ ¼å¼åŒ–å™¨"""
        try:
            formatter_config = self.config["formatters"][formatter_name]
            
            if formatter_config.get("class") == "common.enhanced_logger.StructuredFormatter":
                return StructuredFormatter()
            else:
                return logging.Formatter(
                    fmt=formatter_config.get("format"),
                    datefmt=formatter_config.get("datefmt")
                )
        except Exception as e:
            print(f"åˆ›å»ºæ ¼å¼åŒ–å™¨å¤±è´¥ {formatter_name}: {e}")
            return None
    
    def _create_stats_filter(self):
        """åˆ›å»ºç»Ÿè®¡è¿‡æ»¤å™¨"""
        def stats_filter(record):
            self.stats["total_logs"] += 1
            self.stats["logs_by_level"][record.levelname] += 1
            
            logger_name = record.name
            if logger_name not in self.stats["logs_by_logger"]:
                self.stats["logs_by_logger"][logger_name] = 0
            self.stats["logs_by_logger"][logger_name] += 1
            
            return True
        
        return stats_filter
    
    def get_logger(self, name: str) -> logging.Logger:
        """è·å–æ—¥å¿—å™¨"""
        if name not in self.loggers:
            return self.setup_logger(name)
        return self.loggers[name]
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        uptime = time.time() - self.stats["start_time"]
        
        return {
            **self.stats,
            "uptime_seconds": uptime,
            "logs_per_second": self.stats["total_logs"] / uptime if uptime > 0 else 0,
            "active_loggers": len(self.loggers)
        }


# å…¨å±€æ—¥å¿—ç®¡ç†å™¨å®ä¾‹
_enhanced_log_manager = None

def get_enhanced_log_manager() -> EnhancedLogManager:
    """è·å–å…¨å±€å¢å¼ºæ—¥å¿—ç®¡ç†å™¨å®ä¾‹"""
    global _enhanced_log_manager
    if _enhanced_log_manager is None:
        _enhanced_log_manager = EnhancedLogManager()
    return _enhanced_log_manager


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•å¢å¼ºæ—¥å¿—ç³»ç»Ÿ"""
    manager = get_enhanced_log_manager()
    
    print("ğŸ“ å¢å¼ºæ—¥å¿—ç®¡ç†ç³»ç»Ÿæµ‹è¯•")
    
    # åˆ›å»ºæµ‹è¯•æ—¥å¿—å™¨
    logger = manager.get_logger("test_logger")
    
    # æµ‹è¯•å„ç§æ—¥å¿—çº§åˆ«
    logger.debug("è¿™æ˜¯è°ƒè¯•ä¿¡æ¯")
    logger.info("è¿™æ˜¯æ™®é€šä¿¡æ¯")
    logger.warning("è¿™æ˜¯è­¦å‘Šä¿¡æ¯")
    logger.error("è¿™æ˜¯é”™è¯¯ä¿¡æ¯")
    logger.critical("è¿™æ˜¯ä¸¥é‡é”™è¯¯ä¿¡æ¯")
    
    # æµ‹è¯•ç»“æ„åŒ–æ—¥å¿—
    logger.info("ç”¨æˆ·ç™»å½•", extra={
        'extra_data': {'user_id': '123', 'ip': '192.168.1.1'},
        'trace_id': 'trace-001'
    })
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_stats()
    print(f"\næ—¥å¿—ç»Ÿè®¡:")
    print(f"æ€»æ—¥å¿—æ•°: {stats['total_logs']}")
    print(f"æ´»è·ƒæ—¥å¿—å™¨: {stats['active_loggers']}")
    print(f"è¿è¡Œæ—¶é—´: {stats['uptime_seconds']:.1f}ç§’")
    
    print("\nâœ… å¢å¼ºæ—¥å¿—ç³»ç»Ÿæµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    main()
